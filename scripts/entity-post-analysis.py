#!/usr/bin/env python3
"""
Entity Post-Analysis Script

This script processes entity summary JSON files generated by entity-analysis.py.
It identifies potential duplicate entities (variants of the same real-world entity)
using Google's Gemini AI for clustering based on filenames.

For each identified cluster of duplicates, it:
1. Selects a canonical name (longest variant).
2. Aggregates information (summaries, document IDs, connections) from all variants.
3. Uses Gemini AI again to synthesize a new, comprehensive summary for the canonical entity.
4. Saves the merged entity summary to a new file.
5. Archives the original duplicate files.
"""

import os
import json
import glob
import re
import shutil
from collections import defaultdict
from google.api_core import exceptions as google_exceptions
from google.generativeai import client as genai_client
from google.generativeai import types as genai_types
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define absolute paths (Ensure these match your environment)
ENTITY_SUMMARY_DIR = "/Users/stephenwalker/Code/ecosystem/jfk-files/json/entity_summaries/"
ARCHIVE_DIR_NAME = "archived_duplicates" # Subdirectory within ENTITY_SUMMARY_DIR
ARCHIVE_DIR = os.path.join(ENTITY_SUMMARY_DIR, ARCHIVE_DIR_NAME)

# Entity type to process (e.g., "person-mentioned", "sender", "tag")
ENTITY_TYPE_TO_PROCESS = "person-mentioned"

# Gemini Models
GEMINI_CLUSTER_MODEL = "gemini-2.0-flash" # Model for Hop 1 (Clustering) - Use Flash 2
GEMINI_MERGE_MODEL = "gemini-2.0-flash"   # Model for Hop 2 (Merging) - Use Flash 2
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# Concurrency for Merging Step
MAX_MERGE_WORKERS = 50

# --- Gemini Client Initialization ---
try:
    client = genai_client.Client(api_key=GEMINI_API_KEY)
except Exception as e:
    logging.error(f"Failed to initialize Gemini client: {e}")
    exit(1)

# --- Helper Functions ---

def sanitize_filename(name):
    """Sanitizes a string to be safe for use as a filename part."""
    # Replace spaces with underscores first
    sanitized = name.replace(' ', '_')
    # Remove characters that are problematic in filenames or were removed in entity-analysis.py
    chars_to_remove = ['/', '\\', '.', ',', "'", '[', ']', ':', '?', '*', '<', '>', '|', '"']
    for char in chars_to_remove:
        sanitized = sanitized.replace(char, '')
    # Limit length if necessary (optional)
    # sanitized = sanitized[:100]
    return sanitized

def extract_entity_name_from_filename(filename, entity_type_prefix):
    """
    Extracts the sanitized entity name from the filename.
    Example: "person-mentioned-John_F_Kennedy.json" -> "John_F_Kennedy"
    """
    base_name = os.path.basename(filename)
    if base_name.startswith(entity_type_prefix) and base_name.endswith(".json"):
        # Remove prefix and suffix
        name_part = base_name[len(entity_type_prefix):-len(".json")]
        return name_part
    return None # Return None if format doesn't match

def find_entity_files(directory, entity_type_prefix, archive_subdir_name):
    """Finds entity JSON files, excluding the archive directory."""
    pattern = os.path.join(directory, f"{entity_type_prefix}*.json")
    all_files = glob.glob(pattern)
    # Filter out files within the archive subdirectory
    archive_path_prefix = os.path.join(directory, archive_subdir_name)
    filtered_files = [f for f in all_files if not f.startswith(archive_path_prefix)]
    logging.info(f"Found {len(filtered_files)} files for prefix '{entity_type_prefix}' excluding archives.")
    return filtered_files

def load_entity_data(filepath):
    """Loads JSON data from a single entity file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"File not found during load: {filepath}")
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in file: {filepath}")
    except Exception as e:
        logging.error(f"Error loading {filepath}: {e}")
    return None

def select_canonical_name(cluster_filepaths, entity_type_prefix):
    """Selects the longest entity name from a cluster as the canonical name."""
    longest_name = ""
    for fp in cluster_filepaths:
        name = extract_entity_name_from_filename(fp, entity_type_prefix)
        if name and len(name) > len(longest_name):
            longest_name = name
    # Fallback if no valid name extracted (shouldn't happen with proper filtering)
    if not longest_name and cluster_filepaths:
         # Use the first filename's extracted name as a last resort
        longest_name = extract_entity_name_from_filename(cluster_filepaths[0], entity_type_prefix) or "unknown_entity"
    return longest_name

def aggregate_cluster_data(cluster_filepaths):
    """Aggregates data from all files in a cluster."""
    aggregated = {
        "variant_names": set(),
        "all_summaries": [],
        "unique_doc_ids": set(),
        "unique_connections": set(),
        "total_doc_count": 0, # This will be recalculated based on unique IDs
        "original_filepaths": cluster_filepaths # Keep track for archiving
    }
    entity_type = None # Store the entity type from the first valid file

    for fp in cluster_filepaths:
        data = load_entity_data(fp)
        if data:
            aggregated["variant_names"].add(data.get("entity_name", "Unknown"))
            if not entity_type: # Get entity type from first valid file
                 entity_type = data.get("entity_type")
            aggregated["all_summaries"].append(data.get("summary", ""))
            aggregated["unique_doc_ids"].update(data.get("document_ids", []))
            aggregated["unique_connections"].update(data.get("key_connections", []))
        else:
            logging.warning(f"Could not load data for {fp} during aggregation.")

    aggregated["total_doc_count"] = len(aggregated["unique_doc_ids"])
    aggregated["entity_type"] = entity_type # Add the determined entity type
    return aggregated

def archive_variant_files(cluster_filepaths, archive_dir):
    """Moves files from a cluster into the archive directory."""
    os.makedirs(archive_dir, exist_ok=True)
    for fp in cluster_filepaths:
        try:
            base_name = os.path.basename(fp)
            dest_path = os.path.join(archive_dir, base_name)
            shutil.move(fp, dest_path)
            logging.debug(f"Archived {base_name} to {archive_dir}")
        except FileNotFoundError:
            logging.warning(f"File not found during archive, may have been moved already: {fp}")
        except Exception as e:
            logging.error(f"Error archiving file {fp}: {e}")

def save_merged_entity(merged_data, canonical_name, entity_type, output_dir):
    """Saves the merged entity data to a new JSON file."""
    # Note: canonical_name is already sanitized as it came from extract_entity_name...
    filename = f"meta-{entity_type}-{canonical_name}.json"
    output_path = os.path.join(output_dir, filename)
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, indent=2, ensure_ascii=False)
        logging.info(f"Saved merged entity: {output_path}")
        return output_path
    except Exception as e:
        logging.error(f"Error saving merged entity file {output_path}: {e}")
        return None

# --- Gemini Interaction Functions ---

def get_entity_clusters_from_gemini(filepaths, entity_type_prefix, model_name):
    """
    Uses Gemini (Hop 1) to cluster entity filenames.

    Args:
        filepaths: List of file paths for a specific entity type.
        entity_type_prefix: The prefix string (e.g., "person-mentioned-").
        model_name: The Gemini model name to use for clustering.

    Returns:
        A list of lists, where each inner list contains filepaths
        belonging to the same cluster. Returns None on failure.
    """
    if not filepaths:
        logging.info("No filepaths provided for clustering.")
        return []

    # Create mapping from extracted name to original filepath
    name_to_filepath = {}
    entity_names = []
    for fp in filepaths:
        name = extract_entity_name_from_filename(fp, entity_type_prefix)
        if name:
            # Handle potential name collisions (though unlikely with UUIDs/good sanitization)
            if name in name_to_filepath:
                 logging.warning(f"Duplicate extracted name '{name}' found for files: {name_to_filepath[name]} and {fp}. Using first encountered.")
            else:
                name_to_filepath[name] = fp
                entity_names.append(name)
        else:
            logging.warning(f"Could not extract valid name from: {fp}")

    if not entity_names:
        logging.error("No valid entity names extracted for clustering.")
        return []

    logging.info(f"Requesting clustering for {len(entity_names)} unique entity names.")

    # Construct the prompt for Gemini
    prompt = f"""
    Analyze the following list of entity names extracted from filenames. These entities are all of the type '{ENTITY_TYPE_TO_PROCESS}'.
    Group the names that refer to the exact same real-world entity. Consider variations in titles, initials, abbreviations, and common references.

    Entity Names:
    {json.dumps(entity_names, indent=2)}

    Return your response ONLY as a JSON object containing a single key "clusters".
    The value of "clusters" should be a JSON list of lists. Each inner list represents a cluster of names referring to the same entity.
    Include ALL provided entity names in the output, placing each name in exactly one cluster.
    Names that do not have duplicates should be in their own cluster (a list containing only that single name).

    Example Input Names: ["JFK", "John_F_Kennedy", "President_Kennedy", "Lee_Harvey_Oswald", "Oswald", "CIA"]
    Example JSON Output:
    {{
      "clusters": [
        ["JFK", "John_F_Kennedy", "President_Kennedy"],
        ["Lee_Harvey_Oswald", "Oswald"],
        ["CIA"]
      ]
    }}
    """

    try:
        # Configure Gemini call for JSON output
        generation_config = genai_types.GenerationConfig(
            response_mime_type="application/json",
            temperature=0.1 # Lower temperature for more deterministic clustering
        )
        model = client.get_generative_model(model_name) # Use get_generative_model
        response = model.generate_content(
            contents=prompt,
            generation_config=generation_config
        )

        # Debug: Log the raw response text
        # logging.debug(f"Raw Gemini clustering response: {response.text}")

        # Parse the JSON response
        response_data = json.loads(response.text)
        clustered_names = response_data.get("clusters")

        if not isinstance(clustered_names, list):
            logging.error(f"Gemini clustering response did not contain a valid 'clusters' list. Response: {response.text}")
            return None

        # Convert clustered names back to filepaths
        final_clusters = []
        processed_names = set()
        for name_cluster in clustered_names:
            path_cluster = []
            if not isinstance(name_cluster, list):
                 logging.warning(f"Skipping invalid cluster format in response: {name_cluster}")
                 continue
            for name in name_cluster:
                if name in name_to_filepath:
                    path_cluster.append(name_to_filepath[name])
                    processed_names.add(name)
                else:
                    logging.warning(f"Name '{name}' from Gemini cluster not found in original name map.")
            if path_cluster: # Only add non-empty clusters
                final_clusters.append(path_cluster)

        # Check for names missed by Gemini
        original_names_set = set(entity_names)
        missed_names = original_names_set - processed_names
        if missed_names:
            logging.warning(f"Gemini clustering missed {len(missed_names)} names: {missed_names}. Adding them as individual clusters.")
            for name in missed_names:
                 if name in name_to_filepath:
                      final_clusters.append([name_to_filepath[name]])


        logging.info(f"Successfully clustered into {len(final_clusters)} groups.")
        return final_clusters

    except json.JSONDecodeError:
        logging.error(f"Failed to parse JSON response from Gemini clustering. Response text: {response.text}")
        return None
    except google_exceptions.GoogleAPICallError as e:
         logging.error(f"Gemini API call failed during clustering: {e}")
         return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during Gemini clustering: {e}")
        # Consider logging the prompt here for debugging if it doesn't contain sensitive info
        # logging.error(f"Prompt sent: {prompt}")
        return None


def generate_merged_summary(canonical_name, entity_type, aggregated_data, model_name):
    """
    Uses Gemini (Hop 2) to generate a merged summary for a cluster.

    Args:
        canonical_name: The chosen canonical name (sanitized).
        entity_type: The type of the entity (e.g., "person-mentioned").
        aggregated_data: Dictionary containing 'variant_names', 'all_summaries',
                         'unique_doc_ids', 'unique_connections', 'total_doc_count'.
        model_name: The Gemini model name to use for merging.

    Returns:
        A dictionary containing the merged entity data, or None on failure.
    """
    logging.debug(f"Generating merged summary for canonical name: {canonical_name}")

    # Construct the prompt for Gemini merging
    prompt = f"""
    You are tasked with merging information about the same entity, which appeared under different names or contexts in various documents.

    Canonical Entity Name: {canonical_name} (This is the primary name to use)
    Entity Type: {entity_type}
    Variant Names Encountered: {json.dumps(list(aggregated_data['variant_names']))}
    Total Unique Documents Mentioning This Entity: {aggregated_data['total_doc_count']}

    Individual Summaries from Variant Files:
    ---
    {json.dumps(aggregated_data['all_summaries'], indent=2)}
    ---

    Aggregated List of Document IDs:
    {json.dumps(list(aggregated_data['unique_doc_ids']))}

    Aggregated List of Potential Key Connections (other entities mentioned):
    {json.dumps(list(aggregated_data['unique_connections']))}

    Instructions:
    Based ONLY on the information provided above (individual summaries, connections, document count):
    1. Synthesize a single, comprehensive, and coherent 'summary' paragraph for the canonical entity '{canonical_name}'. Do NOT just concatenate the old summaries; integrate the information smoothly. Focus on the most important aspects revealed across all variants.
    2. Identify the most relevant 'key_connections' from the aggregated list provided. Choose connections that appear significant based on the summaries. Limit this to a reasonable number (e.g., 5-15).
    3. Determine the overall 'significance' of this entity within the context of the source material (e.g., JFK assassination documents), considering the number of documents and the content of the summaries.

    Return your response ONLY as a JSON object with the following fields:
    - entity_name: String (use the canonical_name: "{canonical_name}")
    - entity_type: String (use the provided entity_type: "{entity_type}")
    - document_count: Integer (use the total unique document count: {aggregated_data['total_doc_count']})
    - document_ids: Array of strings (use the aggregated list of unique document IDs)
    - summary: String (the new, synthesized summary paragraph)
    - key_connections: Array of strings (the curated list of significant connections)
    - significance: String (the assessment of the entity's significance)
    - merged_from_variants: Array of strings (list the variant names provided above)

    Example JSON Output Structure:
    {{
      "entity_name": "{canonical_name}",
      "entity_type": "{entity_type}",
      "document_count": {aggregated_data['total_doc_count']},
      "document_ids": ["doc_id_1", "doc_id_2", ...],
      "summary": "A synthesized summary paragraph...",
      "key_connections": ["Connection1", "Connection2", ...],
      "significance": "High/Medium/Low - explanation...",
      "merged_from_variants": ["Variant1", "Variant2", ...]
    }}
    """

    try:
        # Configure Gemini call for JSON output
        generation_config = genai_types.GenerationConfig(
            response_mime_type="application/json",
            temperature=0.3 # Slightly higher temp for synthesis creativity
        )
        model = client.get_generative_model(model_name)
        response = model.generate_content(
            contents=prompt,
            generation_config=generation_config
        )

        # Debug: Log the raw response text
        # logging.debug(f"Raw Gemini merge response for {canonical_name}: {response.text}")

        # Parse the JSON response
        merged_data = json.loads(response.text)

        # --- Data Validation and Refinement ---
        # Ensure essential fields exist and have correct types (basic check)
        if not all(k in merged_data for k in ["entity_name", "entity_type", "document_count", "summary", "key_connections", "significance"]):
             logging.error(f"Merged data for {canonical_name} missing required fields. Response: {response.text}")
             return None

        # Ensure document_ids are included correctly
        merged_data["document_ids"] = list(aggregated_data['unique_doc_ids'])
        # Ensure merged_from_variants are included correctly
        merged_data["merged_from_variants"] = list(aggregated_data['variant_names'])
        # Ensure canonical name and type are set correctly
        merged_data["entity_name"] = canonical_name
        merged_data["entity_type"] = entity_type
        merged_data["document_count"] = aggregated_data['total_doc_count']


        logging.debug(f"Successfully generated merged data for {canonical_name}")
        return merged_data

    except json.JSONDecodeError:
        logging.error(f"Failed to parse JSON response from Gemini merge for {canonical_name}. Response text: {response.text}")
        return None
    except google_exceptions.GoogleAPICallError as e:
         logging.error(f"Gemini API call failed during merge for {canonical_name}: {e}")
         return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during Gemini merge for {canonical_name}: {e}")
        # Consider logging the prompt here for debugging if it doesn't contain sensitive info
        # logging.error(f"Prompt sent: {prompt}")
        return None

# --- Main Workflow ---

def process_cluster(cluster_filepaths, entity_type_prefix, merge_model_name, output_dir, archive_dir):
    """Handles the merging process for a single cluster."""
    if len(cluster_filepaths) <= 1:
        logging.debug(f"Skipping cluster with {len(cluster_filepaths)} file(s): {cluster_filepaths}")
        return None # Nothing to merge

    logging.info(f"Processing cluster with {len(cluster_filepaths)} files: {cluster_filepaths}")

    # 1. Select Canonical Name
    canonical_name = select_canonical_name(cluster_filepaths, entity_type_prefix)
    if not canonical_name:
         logging.error(f"Could not determine canonical name for cluster: {cluster_filepaths}")
         return None # Cannot proceed without a name

    # 2. Aggregate Data
    aggregated_data = aggregate_cluster_data(cluster_filepaths)
    if not aggregated_data or not aggregated_data.get("entity_type"):
         logging.error(f"Failed to aggregate data for cluster with canonical name '{canonical_name}'.")
         return None # Cannot proceed without aggregated data or entity type

    entity_type = aggregated_data["entity_type"] # Get the type determined during aggregation

    # 3. Generate Merged Summary (Hop 2)
    merged_data = generate_merged_summary(canonical_name, entity_type, aggregated_data, merge_model_name)

    if merged_data:
        # 4. Save Merged Entity
        saved_path = save_merged_entity(merged_data, canonical_name, entity_type, output_dir)

        if saved_path:
            # 5. Archive Original Files
            archive_variant_files(cluster_filepaths, archive_dir)
            return saved_path # Return path of the new merged file
        else:
            logging.error(f"Failed to save merged entity for {canonical_name}, originals will not be archived.")
            return None # Failed to save
    else:
        logging.error(f"Failed to generate merged summary for {canonical_name}, originals will not be archived.")
        return None # Failed merge generation

def main():
    """Main function to orchestrate the entity merging process."""
    logging.info("--- Starting Entity Post-Analysis ---")
    logging.info(f"Processing entity type: {ENTITY_TYPE_TO_PROCESS}")
    logging.info(f"Entity summaries source: {ENTITY_SUMMARY_DIR}")
    logging.info(f"Archive directory: {ARCHIVE_DIR}")

    entity_type_prefix = f"{ENTITY_TYPE_TO_PROCESS}-"

    # 1. Find all relevant entity files
    all_files = find_entity_files(ENTITY_SUMMARY_DIR, entity_type_prefix, ARCHIVE_DIR_NAME)
    if not all_files:
        logging.info("No entity files found to process. Exiting.")
        return

    # 2. Get Clusters from Gemini (Hop 1)
    logging.info("Requesting entity clustering from Gemini (Hop 1)...")
    clusters = get_entity_clusters_from_gemini(all_files, entity_type_prefix, GEMINI_CLUSTER_MODEL)

    if clusters is None:
        logging.error("Failed to get clusters from Gemini. Aborting merge process.")
        return
    if not clusters:
        logging.info("No clusters returned or identified by Gemini. Exiting.")
        return

    # Separate clusters needing merging
    clusters_to_merge = [c for c in clusters if len(c) > 1]
    singleton_clusters = [c for c in clusters if len(c) <= 1]

    logging.info(f"Identified {len(clusters_to_merge)} clusters requiring merging.")
    logging.info(f"Identified {len(singleton_clusters)} singleton clusters (no merging needed).")

    if not clusters_to_merge:
        logging.info("No clusters require merging. Exiting.")
        return

    # 3. Process Merging in Parallel (Hop 2)
    logging.info(f"Starting parallel merge process for {len(clusters_to_merge)} clusters using up to {MAX_MERGE_WORKERS} workers...")
    merged_count = 0
    failed_clusters = 0

    # Ensure archive directory exists before starting workers
    os.makedirs(ARCHIVE_DIR, exist_ok=True)

    with ThreadPoolExecutor(max_workers=MAX_MERGE_WORKERS) as executor:
        # Submit tasks
        future_to_cluster = {
            executor.submit(
                process_cluster,
                cluster_filepaths,
                entity_type_prefix,
                GEMINI_MERGE_MODEL,
                ENTITY_SUMMARY_DIR, # Output merged files to the main summary dir
                ARCHIVE_DIR
            ): cluster_filepaths for cluster_filepaths in clusters_to_merge
        }

        # Process results as they complete
        for future in tqdm(as_completed(future_to_cluster), total=len(clusters_to_merge), desc="Merging Clusters"):
            cluster_filepaths = future_to_cluster[future]
            try:
                result = future.result()
                if result:
                    merged_count += 1
                    logging.debug(f"Successfully merged cluster resulting in: {result}")
                else:
                    # Error logged within process_cluster
                    failed_clusters += 1
                    logging.warning(f"Failed to process cluster: {cluster_filepaths}")
            except Exception as exc:
                failed_clusters += 1
                logging.error(f"Cluster {cluster_filepaths} generated an exception: {exc}", exc_info=True) # Log traceback

    logging.info("--- Entity Post-Analysis Complete ---")
    logging.info(f"Successfully merged {merged_count} clusters.")
    if failed_clusters > 0:
        logging.warning(f"Failed to merge {failed_clusters} clusters. Check logs for details.")

if __name__ == "__main__":
    if not GEMINI_API_KEY:
        logging.error("GEMINI_API_KEY environment variable not set.")
    else:
        main()