#!/usr/bin/env python3
"""
Entity Post-Analysis Script

This script processes entity summary JSON files generated by entity-analysis.py.
It identifies potential duplicate entities (variants of the same real-world entity)
using Google's Gemini AI for clustering based on filenames.

For each identified cluster of duplicates, it:
1. Selects a canonical name (longest variant).
2. Aggregates information (summaries, document IDs, connections) from all variants.
3. Uses Gemini AI again to synthesize a new, comprehensive summary for the canonical entity.
4. Saves the merged entity summary to a new file.
5. Archives the original duplicate files.
"""

import os
import json
import glob
import re
import shutil
from collections import defaultdict
from google import genai
from google.genai import types as genai_types
from google.api_core import exceptions as google_exceptions
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define absolute paths (Ensure these match your environment)
ENTITY_SUMMARY_DIR = "/Users/stephenwalker/Code/ecosystem/jfk-files/json/entity_summaries/"
ARCHIVE_DIR_NAME = "archived_duplicates" # Subdirectory within ENTITY_SUMMARY_DIR
ARCHIVE_DIR = os.path.join(ENTITY_SUMMARY_DIR, ARCHIVE_DIR_NAME)

# Entity type to process (e.g., "person-mentioned", "sender", "tag")
ENTITY_TYPE_TO_PROCESS = "person-mentioned"

# Gemini Models
GEMINI_CLUSTER_MODEL = "gemini-2.5-pro-preview-03-25" # Model for Hop 1 (Clustering) - Use 2.5
GEMINI_MERGE_MODEL = "gemini-2.0-flash"   # Model for Hop 2 (Merging) - Use Flash 2
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# Concurrency for Merging Step
MAX_MERGE_WORKERS = 50

# --- Gemini Client Initialization ---
try:
    client = genai.Client(api_key=GEMINI_API_KEY)
except Exception as e:
    logging.error(f"Failed to initialize Gemini client: {e}")
    exit(1)

# --- Helper Functions ---

def sanitize_filename(name):
    """Sanitizes a string to be safe for use as a filename part."""
    # Replace spaces with underscores first
    sanitized = name.replace(' ', '_')
    # Remove characters that are problematic in filenames or were removed in entity-analysis.py
    chars_to_remove = ['/', '\\', '.', ',', "'", '[', ']', ':', '?', '*', '<', '>', '|', '"']
    for char in chars_to_remove:
        sanitized = sanitized.replace(char, '')
    # Limit length if necessary (optional)
    # sanitized = sanitized[:100]
    return sanitized

def extract_entity_name_from_filename(filename, entity_type_prefix):
    """
    Extracts the sanitized entity name from the filename.
    Example: "person-mentioned-John_F_Kennedy.json" -> "John_F_Kennedy"
    """
    base_name = os.path.basename(filename)
    if base_name.startswith(entity_type_prefix) and base_name.endswith(".json"):
        # Remove prefix and suffix
        name_part = base_name[len(entity_type_prefix):-len(".json")]
        return name_part
    return None # Return None if format doesn't match

def find_entity_files(directory, entity_type_prefix, archive_subdir_name):
    """Finds entity JSON files, excluding the archive directory."""
    pattern = os.path.join(directory, f"{entity_type_prefix}*.json")
    all_files = glob.glob(pattern)
    # Filter out files within the archive subdirectory
    archive_path_prefix = os.path.join(directory, archive_subdir_name)
    filtered_files = [f for f in all_files if not f.startswith(archive_path_prefix)]
    logging.info(f"Found {len(filtered_files)} files for prefix '{entity_type_prefix}' excluding archives.")
    return filtered_files

def load_entity_data(filepath):
    """Loads JSON data from a single entity file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"File not found during load: {filepath}")
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in file: {filepath}")
    except Exception as e:
        logging.error(f"Error loading {filepath}: {e}")
    return None

def select_canonical_name(cluster_filepaths, entity_type_prefix):
    """Selects the longest entity name from a cluster as the canonical name."""
    longest_name = ""
    for fp in cluster_filepaths:
        name = extract_entity_name_from_filename(fp, entity_type_prefix)
        if name and len(name) > len(longest_name):
            longest_name = name
    # Fallback if no valid name extracted (shouldn't happen with proper filtering)
    if not longest_name and cluster_filepaths:
         # Use the first filename's extracted name as a last resort
        longest_name = extract_entity_name_from_filename(cluster_filepaths[0], entity_type_prefix) or "unknown_entity"
    return longest_name

def aggregate_cluster_data(cluster_filepaths):
    """Aggregates data from all files in a cluster."""
    aggregated = {
        "variant_names": set(),
        "all_summaries": [],
        "unique_doc_ids": set(),
        "unique_connections": set(),
        "total_doc_count": 0, # This will be recalculated based on unique IDs
        "original_filepaths": cluster_filepaths # Keep track for archiving
    }
    entity_type = None # Store the entity type from the first valid file

    for fp in cluster_filepaths:
        data = load_entity_data(fp)
        if data:
            aggregated["variant_names"].add(data.get("entity_name", "Unknown"))
            if not entity_type: # Get entity type from first valid file
                 entity_type = data.get("entity_type")
            aggregated["all_summaries"].append(data.get("summary", ""))
            aggregated["unique_doc_ids"].update(data.get("document_ids", []))
            aggregated["unique_connections"].update(data.get("key_connections", []))
        else:
            logging.warning(f"Could not load data for {fp} during aggregation.")

    aggregated["total_doc_count"] = len(aggregated["unique_doc_ids"])
    aggregated["entity_type"] = entity_type # Add the determined entity type
    return aggregated

def archive_variant_files(cluster_filepaths, archive_dir):
    """Moves files from a cluster into the archive directory."""
    os.makedirs(archive_dir, exist_ok=True)
    for fp in cluster_filepaths:
        try:
            base_name = os.path.basename(fp)
            dest_path = os.path.join(archive_dir, base_name)
            shutil.move(fp, dest_path)
            logging.debug(f"Archived {base_name} to {archive_dir}")
        except FileNotFoundError:
            logging.warning(f"File not found during archive, may have been moved already: {fp}")
        except Exception as e:
            logging.error(f"Error archiving file {fp}: {e}")

def save_merged_entity(merged_data, canonical_name, entity_type, output_dir):
    """Saves the merged entity data to a new JSON file."""
    # Note: canonical_name is already sanitized as it came from extract_entity_name...
    filename = f"meta-{entity_type}-{canonical_name}.json"
    output_path = os.path.join(output_dir, filename)
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, indent=2, ensure_ascii=False)
        logging.info(f"Saved merged entity: {output_path}")
        return output_path
    except Exception as e:
        logging.error(f"Error saving merged entity file {output_path}: {e}")
        return None

# --- Gemini Interaction Functions ---

def get_entity_clusters_from_gemini(filepaths, entity_type_prefix, model_name):
    """
    Prepares the Gemini clustering payload and dumps it to a file instead of making the API call.

    Args:
        filepaths: List of file paths for a specific entity type.
        entity_type_prefix: The prefix string (e.g., "person-mentioned-").
        model_name: The Gemini model name that would have been used.

    Returns:
        Path to the dumped payload file.
    """
    if not filepaths:
        logging.info("No filepaths provided for clustering.")
        return []

    # Create mapping from extracted name to original filepath
    name_to_filepath = {}
    entity_names = []
    for fp in filepaths:
        name = extract_entity_name_from_filename(fp, entity_type_prefix)
        if name:
            # Handle potential name collisions (though unlikely with UUIDs/good sanitization)
            if name in name_to_filepath:
                 logging.warning(f"Duplicate extracted name '{name}' found for files: {name_to_filepath[name]} and {fp}. Using first encountered.")
            else:
                name_to_filepath[name] = fp
                entity_names.append(name)
        else:
            logging.warning(f"Could not extract valid name from: {fp}")

    if not entity_names:
        logging.error("No valid entity names extracted for clustering.")
        return []

    logging.info(f"Preparing clustering payload for {len(entity_names)} unique entity names.")

    # Construct the prompt for Gemini
    prompt = f"""
    Analyze the following list of entity names extracted from filenames. These entities are all of the type '{ENTITY_TYPE_TO_PROCESS}'.
    Group the names that refer to the exact same real-world entity. Consider variations in titles, initials, abbreviations, and common references.

    Entity Names:
    {json.dumps(entity_names, indent=2)}

    Return your response ONLY as a JSON object containing a single key "clusters".
    The value of "clusters" should be a JSON list of lists. Each inner list represents a cluster of names referring to the same entity.
    Include ALL provided entity names in the output, placing each name in exactly one cluster.
    Names that do not have duplicates should be in their own cluster (a list containing only that single name).

    Example Input Names: ["JFK", "John_F_Kennedy", "President_Kennedy", "Lee_Harvey_Oswald", "Oswald", "CIA"]
    Example JSON Output:
    {{
      "clusters": [
        ["JFK", "John_F_Kennedy", "President_Kennedy"],
        ["Lee_Harvey_Oswald", "Oswald"],
        ["CIA"]
      ]
    }}
    """

    # Prepare the payload
    payload = {
        "model": model_name,
        "prompt": prompt,
        "config": {
            "temperature": 0.1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 65536 if "2.5" in model_name else 8192,
            "response_mime_type": "application/json"
        }
    }

    # Dump the payload to a file
    output_file = "gemini_clustering_payload.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2)
        logging.info(f"Dumped Gemini clustering payload to {output_file}")
        return output_file
    except Exception as e:
        logging.error(f"Failed to dump payload to file: {e}")
        return None

def generate_merged_summary(canonical_name, entity_type, aggregated_data, model_name):
    """
    Prepares the Gemini merging payload and dumps it to a file instead of making the API call.

    Args:
        canonical_name: The chosen canonical name (sanitized).
        entity_type: The type of the entity (e.g., "person-mentioned").
        aggregated_data: Dictionary containing 'variant_names', 'all_summaries',
                         'unique_doc_ids', 'unique_connections', 'total_doc_count'.
        model_name: The Gemini model name that would have been used.

    Returns:
        Path to the dumped payload file.
    """
    logging.debug(f"Preparing merge payload for canonical name: {canonical_name}")

    # Construct the prompt for Gemini merging
    prompt = f"""
    You are tasked with merging information about the same entity, which appeared under different names or contexts in various documents.

    Canonical Entity Name: {canonical_name} (This is the primary name to use)
    Entity Type: {entity_type}
    Variant Names Encountered: {json.dumps(list(aggregated_data['variant_names']))}
    Total Unique Documents Mentioning This Entity: {aggregated_data['total_doc_count']}

    Individual Summaries from Variant Files:
    ---
    {json.dumps(aggregated_data['all_summaries'], indent=2)}
    ---

    Aggregated List of Document IDs:
    {json.dumps(list(aggregated_data['unique_doc_ids']))}

    Aggregated List of Potential Key Connections (other entities mentioned):
    {json.dumps(list(aggregated_data['unique_connections']))}

    Instructions:
    Based ONLY on the information provided above (individual summaries, connections, document count):
    1. Synthesize a single, comprehensive, and coherent 'summary' paragraph for the canonical entity '{canonical_name}'. Do NOT just concatenate the old summaries; integrate the information smoothly. Focus on the most important aspects revealed across all variants.
    2. Identify the most relevant 'key_connections' from the aggregated list provided. Choose connections that appear significant based on the summaries. Limit this to a reasonable number (e.g., 5-15).
    3. Determine the overall 'significance' of this entity within the context of the source material (e.g., JFK assassination documents), considering the number of documents and the content of the summaries.

    Return your response ONLY as a JSON object with the following fields:
    - entity_name: String (use the canonical_name: "{canonical_name}")
    - entity_type: String (use the provided entity_type: "{entity_type}")
    - document_count: Integer (use the total unique document count: {aggregated_data['total_doc_count']})
    - document_ids: Array of strings (use the aggregated list of unique document IDs)
    - summary: String (the new, synthesized summary paragraph)
    - key_connections: Array of strings (the curated list of significant connections)
    - significance: String (the assessment of the entity's significance)
    - merged_from_variants: Array of strings (list the variant names provided above)

    Example JSON Output Structure:
    {{
      "entity_name": "{canonical_name}",
      "entity_type": "{entity_type}",
      "document_count": {aggregated_data['total_doc_count']},
      "document_ids": ["doc_id_1", "doc_id_2", ...],
      "summary": "A synthesized summary paragraph...",
      "key_connections": ["Connection1", "Connection2", ...],
      "significance": "High/Medium/Low - explanation...",
      "merged_from_variants": ["Variant1", "Variant2", ...]
    }}
    """

    # Prepare the payload
    payload = {
        "model": model_name,
        "prompt": prompt,
        "config": {
            "temperature": 0.3,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 65536 if "2.5" in model_name else 8192,
            "response_mime_type": "application/json"
        }
    }

    # Dump the payload to a file
    output_file = f"gemini_merge_payload_{canonical_name}.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2)
        logging.info(f"Dumped Gemini merge payload to {output_file}")
        return output_file
    except Exception as e:
        logging.error(f"Failed to dump payload to file: {e}")
        return None

def process_cluster(cluster_filepaths, entity_type_prefix, merge_model_name, output_dir, archive_dir):
    """Handles the merging process for a single cluster."""
    if len(cluster_filepaths) <= 1:
        logging.debug(f"Skipping cluster with {len(cluster_filepaths)} file(s): {cluster_filepaths}")
        return None # Nothing to merge

    logging.info(f"Processing cluster with {len(cluster_filepaths)} files: {cluster_filepaths}")

    # 1. Select Canonical Name
    canonical_name = select_canonical_name(cluster_filepaths, entity_type_prefix)
    if not canonical_name:
         logging.error(f"Could not determine canonical name for cluster: {cluster_filepaths}")
         return None # Cannot proceed without a name

    # 2. Aggregate Data
    aggregated_data = aggregate_cluster_data(cluster_filepaths)
    if not aggregated_data or not aggregated_data.get("entity_type"):
         logging.error(f"Failed to aggregate data for cluster with canonical name '{canonical_name}'.")
         return None # Cannot proceed without aggregated data or entity type

    entity_type = aggregated_data["entity_type"] # Get the type determined during aggregation

    # 3. Generate Merged Summary Payload
    payload_file = generate_merged_summary(canonical_name, entity_type, aggregated_data, merge_model_name)
    
    if payload_file:
        logging.info(f"Successfully generated merge payload file: {payload_file}")
        return payload_file
    else:
        logging.error(f"Failed to generate merge payload for {canonical_name}")
        return None

def main():
    """Main function to orchestrate the entity merging process."""
    logging.info("--- Starting Entity Post-Analysis ---")
    logging.info(f"Processing entity type: {ENTITY_TYPE_TO_PROCESS}")
    logging.info(f"Entity summaries source: {ENTITY_SUMMARY_DIR}")

    entity_type_prefix = f"{ENTITY_TYPE_TO_PROCESS}-"

    # 1. Find all relevant entity files
    all_files = find_entity_files(ENTITY_SUMMARY_DIR, entity_type_prefix, ARCHIVE_DIR_NAME)
    if not all_files:
        logging.info("No entity files found to process. Exiting.")
        return

    # 2. Generate Clustering Payload
    logging.info("Generating clustering payload...")
    payload_file = get_entity_clusters_from_gemini(all_files, entity_type_prefix, GEMINI_CLUSTER_MODEL)

    if payload_file:
        logging.info(f"Successfully generated clustering payload file: {payload_file}")
    else:
        logging.error("Failed to generate clustering payload. Exiting.")
        return

if __name__ == "__main__":
    if not GEMINI_API_KEY:
        logging.error("GEMINI_API_KEY environment variable not set.")
    else:
        main()